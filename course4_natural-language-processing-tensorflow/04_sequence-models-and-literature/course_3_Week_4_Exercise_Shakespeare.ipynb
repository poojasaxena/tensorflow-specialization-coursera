{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-15 10:45:54--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.186.144, 142.250.185.176, 142.250.186.80, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.186.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 93578 (91K) [text/plain]\n",
      "Saving to: ‘/tmp/sonnets.txt’\n",
      "\n",
      "/tmp/sonnets.txt    100%[===================>]  91,38K  --.-KB/s    in 0,05s   \n",
      "\n",
      "2021-12-15 10:45:55 (1,68 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
    "    -O /tmp/sonnets.txt\n",
    "data = open('/tmp/sonnets.txt').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 10:45:55.902288: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 100)           321100    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1605)              162105    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3211)              5156866   \n",
      "=================================================================\n",
      "Total params: 6,101,671\n",
      "Trainable params: 6,101,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 10:45:57.366188: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "484/484 [==============================] - 39s 69ms/step - loss: 6.9116 - accuracy: 0.0242\n",
      "Epoch 2/100\n",
      "484/484 [==============================] - 46s 96ms/step - loss: 6.5018 - accuracy: 0.0217\n",
      "Epoch 3/100\n",
      "484/484 [==============================] - 45s 92ms/step - loss: 6.3987 - accuracy: 0.0240\n",
      "Epoch 4/100\n",
      "484/484 [==============================] - 40s 84ms/step - loss: 6.2724 - accuracy: 0.0294\n",
      "Epoch 5/100\n",
      "484/484 [==============================] - 42s 87ms/step - loss: 6.1773 - accuracy: 0.0362\n",
      "Epoch 6/100\n",
      "484/484 [==============================] - 44s 90ms/step - loss: 6.0967 - accuracy: 0.0375\n",
      "Epoch 7/100\n",
      "484/484 [==============================] - 44s 92ms/step - loss: 6.0201 - accuracy: 0.0408\n",
      "Epoch 8/100\n",
      "484/484 [==============================] - 45s 93ms/step - loss: 5.9409 - accuracy: 0.0456\n",
      "Epoch 9/100\n",
      "484/484 [==============================] - 43s 89ms/step - loss: 5.8384 - accuracy: 0.0521\n",
      "Epoch 10/100\n",
      "484/484 [==============================] - 43s 89ms/step - loss: 5.7301 - accuracy: 0.0584\n",
      "Epoch 11/100\n",
      "484/484 [==============================] - 45s 92ms/step - loss: 5.6264 - accuracy: 0.0640\n",
      "Epoch 12/100\n",
      "484/484 [==============================] - 45s 93ms/step - loss: 5.5217 - accuracy: 0.0694\n",
      "Epoch 13/100\n",
      "484/484 [==============================] - 45s 92ms/step - loss: 5.4198 - accuracy: 0.0758\n",
      "Epoch 14/100\n",
      "484/484 [==============================] - 43s 88ms/step - loss: 5.3204 - accuracy: 0.0797\n",
      "Epoch 15/100\n",
      "484/484 [==============================] - 43s 89ms/step - loss: 5.2187 - accuracy: 0.0888\n",
      "Epoch 16/100\n",
      "484/484 [==============================] - 43s 89ms/step - loss: 5.1194 - accuracy: 0.0935\n",
      "Epoch 17/100\n",
      "484/484 [==============================] - 46s 94ms/step - loss: 5.0221 - accuracy: 0.1037\n",
      "Epoch 18/100\n",
      "484/484 [==============================] - 48s 99ms/step - loss: 4.9190 - accuracy: 0.1105\n",
      "Epoch 19/100\n",
      "119/484 [======>.......................] - ETA: 33s - loss: 4.8007 - accuracy: 0.1229"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, label, epochs=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredict_output = model.predict(token_list)\n",
    "    predicted = np.argmax(predict_output, axis=1) #model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python@3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
