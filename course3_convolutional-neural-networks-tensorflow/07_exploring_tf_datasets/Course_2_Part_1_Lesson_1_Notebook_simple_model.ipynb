{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thorough-greeting",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "**tf.data: Build TensorFlow input pipelines**\n",
    "\n",
    "The *tf.data* API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. \n",
    "\n",
    "The *tf.data* API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations.\n",
    "\n",
    "The *tf.data* API introduces a *tf.data.Dataset* abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\n",
    "\n",
    "In order to use a Dataset we need three steps:\n",
    "1. **Importing Data**: Create a Dataset instance from some data\n",
    "2. **Create a Iterator**: By using the created dataset to make an iterator instance to iterate through the dataset\n",
    "3. **Consuming Data**: By using the created iterator we can get the elements from the dataset to feed the model.\n",
    "\n",
    "There are two distinct ways to import/create a dataset:\n",
    "1. A data source constructs a Dataset from data stored in memory or in one or more files.\n",
    "2. A data transformation constructs a dataset from one or more *tf.data.Dataset* objects.\n",
    "\n",
    "Refernz:\n",
    "1. [how to use dataset in tensor-flow](https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chronic-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "working-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-warehouse",
   "metadata": {},
   "source": [
    "## 2. Importing Data\n",
    "we first need some data to put inside our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-destiny",
   "metadata": {},
   "source": [
    "### 2.1 From Numpy\n",
    "This is the common case, we have a numpy array and we want to pass it to tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "grateful-technique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1984, 2, 3, 4, 7, 23]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "welsh-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "2\n",
      "3\n",
      "4\n",
      "7\n",
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(elem.numpy()) for elem in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "religious-webcam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "2\n",
      "3\n",
      "4\n",
      "7\n",
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Or by explicitly creating a Python iterarot using *iter* and \n",
    "# consuming its elemenet using *next*\n",
    "it=iter(dataset)\n",
    "[print(j.numpy()) for j in it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "different-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n"
     ]
    }
   ],
   "source": [
    "print(dataset.reduce(0, lambda state, value: state+value).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "current-halifax",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([1984, 2, 3, 4, 7, 23])\n",
    "dataset_ = tf.data.Dataset.from_tensor_slices(y)\n",
    "dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acquired-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "2\n",
      "3\n",
      "4\n",
      "7\n",
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(elem.numpy()) for elem in dataset_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "descending-brake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "2\n",
      "3\n",
      "4\n",
      "7\n",
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Or by explicitly creating a Python iterarot using *iter* and \n",
    "# consuming its elemenet using *next*\n",
    "it=iter(dataset_)\n",
    "[print(j.numpy()) for j in it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "violent-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n"
     ]
    }
   ],
   "source": [
    "print(dataset_.reduce(np.array(0), lambda state, value: state+value).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "treated-studio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (4,), types: tf.float64>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.sample((200,4))\n",
    "dataset_2 = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fifty-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((2,), (1,)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "dataset_3 = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "dataset_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-thailand",
   "metadata": {},
   "source": [
    "## 3. Reading input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-toronto",
   "metadata": {},
   "source": [
    "### 3.1 Consuming NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "therapeutic-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "living-eagle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "center-blake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "romantic-collins",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "psychological-bidder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "stone-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "viral-display",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "accredited-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dressed-louisiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-quantum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
